{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb412f7-70b6-4823-8bd3-1c2df0966374",
   "metadata": {},
   "source": [
    "# Regex exos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca108ace-b96e-405c-8125-d36863d230f1",
   "metadata": {},
   "source": [
    "## Exo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27822b66-95cc-4289-a07e-10ac6b64b4f2",
   "metadata": {},
   "source": [
    "### Problem: Write a regex to match the word \"hello\" in a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f790a64-5510-468c-9aa7-0a4dfe3efd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 5), match='hello'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"hello world\"\n",
    "reg_ex = r\"(hello)\"\n",
    "print(re.match(reg_ex, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8171df1-6829-47e1-940c-69145c7ca11b",
   "metadata": {},
   "source": [
    "##  Exo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30793d5-2d35-46fa-abe3-69f6e855f2c2",
   "metadata": {},
   "source": [
    "### Problem: Write a regex to find all digits in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "739bf31a-efc7-406e-98a0-8e221326135c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12345']\n"
     ]
    }
   ],
   "source": [
    "text = \"My phone number is 12345.\"\n",
    "reg_ex = r\"(\\d+)\" # no digit will excute as the number is not at the start of the string\n",
    "print(re.findall(reg_ex, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0a141c-cef9-4e5c-9eb4-1de950a7c3b2",
   "metadata": {},
   "source": [
    "## Exo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5254263a-9dfc-4138-915c-38ac102fd258",
   "metadata": {},
   "source": [
    "### Problem : Write a regex to extract all words that end with \"ing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eaabe29-febd-4ad9-9126-dfcdf9eae0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'jumping', 'singing']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am ingahmed running and jumping while singing.\"\n",
    " # any string end with ing\n",
    " # r is f\n",
    " # using \\b to match the string\n",
    "reg_ex = r\"\\b\\w+ing\\b\"\n",
    "\n",
    "print(re.findall(reg_ex, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd8d14-dab3-4064-aafe-10ed425e438e",
   "metadata": {},
   "source": [
    "# Tokenization Exos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79cb44-46ee-4f81-a55b-51c957957a0b",
   "metadata": {},
   "source": [
    "## Exo 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0403b3a0-1059-413e-ae32-680a82387b4b",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that tokenizes a simple sentence into individual words using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5025ca7-665b-4c67-bf04-a6f87baca36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a6bd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# punkt for the tokenizer\n",
    "# importing the package is not sufficene to use it you must need to download the package part you need for the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c65d8fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'still', 'running', 'and', 'jumping', 'and', 'playing', 'football', '.', 'My', 'name', 'is', 'ahmed', 'hossam', 'elsayed']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I am still running and jumping and playing football. My name is ahmed hossam elsayed\"\n",
    "tokenized_txt = nltk.word_tokenize(test_sentence)\n",
    "print(tokenized_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b85f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e17047f7-7fd5-4f44-a79b-c4642dc4bb7e",
   "metadata": {},
   "source": [
    "## Exo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d87b0-3b00-4768-8396-b653597ce37a",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that tokenizes a paragraph into individual sentences using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19cd2185-5ac2-4e51-b4b4-2ab14ceb288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am still running and jumping and playing football.', 'My name is ahmed hossam elsayed']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sent = nltk.sent_tokenize(test_sentence)\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad3e75-dac1-46a7-8e8b-3dfdb4c3e336",
   "metadata": {},
   "source": [
    "## Exo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05bd627-7927-4ff7-b777-1b8cf76e6edf",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that tokenizes a sentence into individual words using NLTK. Then, use a regular expression with grouping to extract pairs of the format \"Year-Month\" (e.g., \"2023-08\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b06aab06-526e-47cd-bb4b-ddb187a2964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The project started in 2022-05 and was completed by 2023-08.', 'My name is ahmed hossam']\n",
      "[('2022', '05'), ('2023', '08')]\n"
     ]
    }
   ],
   "source": [
    "special_sentence = \"The project started in 2022-05 and was completed by 2023-08. My name is ahmed hossam\"\n",
    "# tokenization first \n",
    "tokenized_sent = nltk.sent_tokenize(special_sentence)\n",
    "print(tokenized_sent)\n",
    "\n",
    "optimized_reg_ex = r\"(\\d{4})-(\\d{2})\"\n",
    "\n",
    "# wroking with the tokenized sentence\n",
    "# date_reg_ex = r'\\b[4]\\b-\\b[0-9]\\b'\n",
    "print(re.findall(optimized_reg_ex, tokenized_sent[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c633402f-a243-46e1-9a29-457912dc3ce9",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization exos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820abc5b-e78f-4c35-9d08-a4fe675dc053",
   "metadata": {},
   "source": [
    "## Exo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28570502-5457-4dc9-b8fb-7146a111ec0d",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that uses the PorterStemmer to stem a list of words. Display the original words and their stemmed forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12075573-93fc-4030-8eba-ccdf28098edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'jump', 'easili', 'faster', 'runner']\n"
     ]
    }
   ],
   "source": [
    "# import porter stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "# take an object from the class\n",
    "porter = PorterStemmer()\n",
    "words = [\"running\", \"jumps\", \"easily\", \"faster\", \"runner\"]\n",
    "comp_words = [porter.stem(word) for word in words]\n",
    "print(comp_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c037ca-fe19-4c9d-8922-d505cc588653",
   "metadata": {},
   "source": [
    "## Exo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2a93e-8b87-46a4-8b6f-3c971a93b106",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that uses the WordNetLemmatizer to lemmatize a list of words, considering both nouns and verbs. Display the original words and their lemmatized forms. note that for pos n stands for noun and v for Verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a957a11b-3b9d-49b7-a838-ec3aeff8fd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# List of words to lemmatize\n",
    "words = [\"running\", \"jumps\", \"leaving\", \"better\",\"easily\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c1e2162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'jump', 'leaving', 'better', 'easily']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# Create the lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "comb_output = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(comb_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58a5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36567406-6b0a-4040-b27c-76ffc4d7d2a5",
   "metadata": {},
   "source": [
    "## Exo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0fbc56-9144-4018-84d8-f856a3c8a1ea",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that compares the results of stemming and lemmatization for a list of words. Analyze cases where the results differ significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a805533b-1e7e-48f4-b305-e5d83a839148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stability and perpexlity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5674355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be39985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f05453a-e853-49ba-9dd9-d7d69fc49903",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452e7b0-ccce-4e2e-b2b1-cc6f670e064f",
   "metadata": {},
   "source": [
    "## Exo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a1554-f648-4f2e-8777-b8cc509cca46",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that tags the parts of speech for each word in a simple sentence using spaCy. Display the word along with its POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfa320bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for both stemming(step1) and lemmatization(step2) then use the final ouptut for the final tokenization process\n",
    "# lemetization is better should no noun or verb\n",
    "# loop throug the POS then according to it use the lemmatization for better context results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "388edf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x30ff07940>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "# so you can load after using \"python -m spacy download en_core_web_sm\"\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2c0d737-ca04-4270-9838-ebba4874097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "# uisng spacy to tokenize the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1249b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for POS use senetence by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95185ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94ccea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_comp_out = [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3665067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "print(list_comp_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ab1d7-b5ee-4436-9d85-673e4b422fa3",
   "metadata": {},
   "source": [
    "## Exo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6d3df-9efc-4f8f-9f03-291d868e45fa",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that tags the parts of speech for each word in a paragraph using spaCy. Then, count how many nouns, verbs, adjectives, and adverbs are present in the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e93cce05-f42c-437d-97d7-298b6df31946",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"The quick brown fox jumps over the lazy dog. The fox is very quick and agile.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120cfd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee3d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "926aceb5-3123-4a28-a3b6-a5feb57c8011",
   "metadata": {},
   "source": [
    "## Exo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc72f0-81aa-4cac-ba96-46f0a833ebb8",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that tags the parts of speech for each word in a sentence using spaCy. Then, extract and display all the nouns and verbs separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f6fff90-d532-4150-9bc6-d1363b32b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The programmer quickly wrote code and fixed bugs in the software.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a30b06-7b61-4f82-adfa-f1f2f9e1405b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
